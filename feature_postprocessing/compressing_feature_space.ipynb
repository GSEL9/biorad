{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nrrd\n",
    "\n",
    "from ioutil import sample_paths\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_regexes(X):\n",
    "    \n",
    "    col_regexes = []\n",
    "    for col in X.columns:\n",
    "        items = col.split('_')\n",
    "        if 'bins' in items[-1]:\n",
    "            col_regexes.append(('_').join(items[1:-1]))\n",
    "        else:\n",
    "            col_regexes.append(('_').join(items[1:]))\n",
    "\n",
    "    return np.unique(col_regexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_labels(labels, pref='CT'):\n",
    "    \n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "\n",
    "        items = label.split('_')\n",
    "\n",
    "        if len(items) == 3:\n",
    "            if items[1] == 'firstorder':\n",
    "                kind = 'First Order'\n",
    "            else:\n",
    "                kind = items[1].upper()\n",
    "                \n",
    "            if len(items[2]) > 20:\n",
    "                new_labels.append(f'{pref} {kind}\\n{items[2]}')\n",
    "            else:\n",
    "                new_labels.append(f'{pref} {kind} {items[2]}')\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Cannot handle label {label}')\n",
    "    \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_categories(labels):\n",
    "    \"\"\"Process raw feature labels.\"\"\"\n",
    "    prep_labels = []\n",
    "    for label in labels:\n",
    "        if 'shape' in label:\n",
    "            prep_labels.append('Shape')\n",
    "        elif 'PETparam' in label:\n",
    "            prep_labels.append('PET Parameter')\n",
    "        elif 'firstorder' in label:\n",
    "            prep_labels.append('First Order')\n",
    "        elif 'glcm' in label:\n",
    "            prep_labels.append('GLCM')\n",
    "        elif 'gldm' in label:\n",
    "            prep_labels.append('GLDM')\n",
    "        elif 'glrlm' in label:\n",
    "            prep_labels.append('GLRLM')\n",
    "        elif 'glszm' in label:\n",
    "            prep_labels.append('GLSZM')\n",
    "        elif 'ngtdm' in label:\n",
    "            prep_labels.append('NGTDM')\n",
    "        else:\n",
    "            prep_labels.append('Clinical')\n",
    "    return prep_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc(Y):\n",
    "    \"\"\"Calculate intra-class correlation coefficient (ICC).\n",
    "\n",
    "    Reference:\n",
    "        Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: uses in\n",
    "        assessing rater reliability. Psychological bulletin, 86(2), 420.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Data matrix with observations on rows\n",
    "            and measurements on columns.\n",
    "\n",
    "    Returns:\n",
    "        (float): Intraclass correlation coefficient.\n",
    "\n",
    "    \"\"\"\n",
    "    n, k = np.shape(Y)\n",
    "\n",
    "    # Degrees of Freedom\n",
    "    dfc = k - 1\n",
    "    dfe = (n - 1) * (k-1)\n",
    "    dfr = n - 1\n",
    "\n",
    "    # Sum Square Total\n",
    "    Y_avg = np.mean(Y)\n",
    "    SST = np.sum((Y - Y_avg) ** 2)\n",
    "\n",
    "    # Create the design matrix for the different levels:\n",
    "    # * Sessions:\n",
    "    x = np.kron(np.eye(k), np.ones((n, 1)))\n",
    "    # * Subjects:\n",
    "    x0 = np.tile(np.eye(n), (k, 1))\n",
    "    X = np.hstack([x, x0])\n",
    "\n",
    "    # Sum Square Error\n",
    "    predicted_Y = np.dot(\n",
    "        np.dot(np.dot(X, np.linalg.pinv(np.dot(X.T, X))), X.T), Y.flatten('F')\n",
    "    )\n",
    "    residuals = Y.flatten('F') - predicted_Y\n",
    "    SSE = np.sum(residuals ** 2)\n",
    "\n",
    "    MSE = SSE / dfe\n",
    "\n",
    "    # Sum square column effect - between colums\n",
    "    SSC = np.sum((np.mean(Y, axis=0) - Y_avg) ** 2) * n\n",
    "    MSC = SSC / dfc / n\n",
    "\n",
    "    # Sum Square subject effect - between rows/subjects\n",
    "    SSR = SST - SSC - SSE\n",
    "    MSR = SSR / dfr\n",
    "\n",
    "    # ICC(3,1) = (mean square subject - mean square error) /\n",
    "    # (mean square subject + (k-1)*mean square error)\n",
    "    ICC = (MSR - MSE) / (MSR + (k-1) * MSE)\n",
    "\n",
    "    return ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ng: Number of image intensities.\n",
    "hassan_gl_transforms = {\n",
    "    'original_glcm_DifferenceEntropy': lambda Ng, feature: feature / np.log(Ng ** 2),\n",
    "    'original_glcm_JointEntropy': lambda Ng, feature: feature / np.log(Ng ** 2),\n",
    "    'original_glcm_SumEntropy': lambda Ng, feature: feature * Ng,\n",
    "    'original_glcm_Contrast': lambda Ng, feature: feature / (Ng ** 2),\n",
    "    'original_glcm_DifferenceVariance': lambda Ng, feature: feature / (Ng ** 2),\n",
    "    'original_glcm_SumAverage': lambda Ng, feature: feature / Ng,\n",
    "    'original_glcm_DifferenceAverage': lambda Ng, feature: feature / Ng,\n",
    "    'original_glrlm_GrayLevelNonUniformity': lambda Ng, feature: feature * Ng,\n",
    "    'original_glrlm_HighGrayLevelRunEmphasis': lambda Ng, feature: feature / (Ng ** 2),\n",
    "    'original_glrlm_ShortRunHighGrayLevelEmphasis': lambda Ng, feature: feature / (Ng ** 2),\n",
    "    'original_ngtdm_Contrast': lambda Ng, feature: feature / Ng,\n",
    "    'original_ngtdm_Complexity': lambda Ng, feature: feature / (Ng ** 3),\n",
    "    'original_ngtdm_Strength': lambda Ng, feature: feature / (Ng ** 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nv: Number of voxels in ROI.\n",
    "hassan_roi_transforms = {\n",
    "    'original_firstorder_Energy': lambda Nv, feature: feature * Nv,\n",
    "    'original_firstorder_Entropy': lambda Nv, feature: feature / np.log(Nv),\n",
    "    'original_firstorder_TotalEnergy': lambda Nv, feature: feature / Nv,\n",
    "    'original_glcm_Contrast': lambda Nv, feature: feature / Nv,\n",
    "    'original_glcm_InverseVariance': lambda Nv, feature: feature / Nv,\n",
    "    'original_glcm_JointAverage': lambda Nv, feature: feature / Nv,\n",
    "    'original_glrlm_GrayLevelNonUniformity': lambda Nv, feature: feature / Nv,\n",
    "    'original_ngtdm_Coarsness': lambda Nv, feature: feature * Nv,\n",
    "    'original_ngtdm_Strength': lambda Nv, feature: feature * Nv,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_from_hassan_modified(X, gl_bins):\n",
    "    \"\"\"Apply Hassan transform and record ICC for original and\n",
    "    transformed features.\n",
    "\n",
    "    X (pandas.DataFrame):\n",
    "    gl_bins (array-like):\n",
    "\n",
    "    Returns:\n",
    "        (pandas.DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "    icc_orig_feat = np.zeros(len(hassan_gl_transforms.keys()))\n",
    "    icc_norm_feat = np.zeros(len(hassan_gl_transforms.keys()))\n",
    "    for num, (key, transform) in enumerate(hassan_gl_transforms.items()):\n",
    "\n",
    "        feats = X.filter(regex=key)\n",
    "        #print(feats.head())\n",
    "        X_transf = np.zeros_like(feats)\n",
    "        for bin_num, (col, nbins) in enumerate(zip(feats.columns, gl_bins)):\n",
    "            X_transf[:, bin_num] = transform(nbins, feats.loc[:, col].values)\n",
    "\n",
    "        icc_orig_feat[num] = icc(feats.values)\n",
    "        icc_norm_feat[num] = icc(X_transf)\n",
    "\n",
    "    df_icc_orig_feat = pd.DataFrame(\n",
    "        icc_orig_feat, index=hassan_gl_transforms.keys(), columns=['Score']\n",
    "    )\n",
    "    df_icc_orig_feat['Kind'] = ['Original'] * len(hassan_gl_transforms.keys())\n",
    "\n",
    "    df_icc_norm_feat = pd.DataFrame(\n",
    "        icc_norm_feat, index=hassan_gl_transforms.keys(), columns=['Score']\n",
    "    )\n",
    "    df_icc_norm_feat['Kind'] = ['Modified'] * len(hassan_gl_transforms.keys())\n",
    "    df_icc = pd.concat((df_icc_orig_feat, df_icc_norm_feat), axis=0)\n",
    "\n",
    "    df_icc.sort_values(by=['Score'])\n",
    "\n",
    "    return df_icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICC_THRESH = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin widths Z-scored CT stacks: Original images.\n",
    "width32 = 0.1351010101010101\n",
    "width64 = 0.06755050505050506\n",
    "width128 = 0.03377525252525253\n",
    "\n",
    "path_to_ct = './../../data_source/images/ct_nrrd/'\n",
    "path_to_ct_masks = './../../data_source/images/masks_nrrd/'\n",
    "path_to_features = './../../data_source/to_analysis/original_images/all_features_original_images.csv'\n",
    "path_to_target = './../../data_source/to_analysis/original_images/dfs_original_images.csv'\n",
    "path_hassan_icc_ct = './../../figures/compressing_feature_space/orig_images_hassan_icc_ct_texture.png'\n",
    "path_hassan_icc_pet = './../../figures/compressing_feature_space/orig_images_hassan_icc_pet_texture.png'\n",
    "path_to_icc = './../../data_source/to_analysis/compressed_features/all_features_orig_images_icc.csv'\n",
    "path_scc_hassan_dropped =  './../../figures/compressing_feature_space/orig_images_scc_hassan_dropped_radiom_feats.png'\n",
    "path_scc_hassan = './../../figures/compressing_feature_space/orig_images_scc_hassan_radiom_feats.png'\n",
    "path_red_dropped = './../../data_source/to_analysis/compressed_features/all_features_orig_images_icc_dropped.csv'\n",
    "path_pet_params_corr = './../../figures/compressing_feature_space/orig_images_pet_params_corr.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get number of GL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ct_stacks = sample_paths(path_to_ct, path_to_ct_masks, target_format='nrrd')\n",
    "\n",
    "gl_32bins = np.zeros(len(path_ct_stacks))\n",
    "gl_64bins = np.zeros(len(path_ct_stacks))\n",
    "gl_128bins = np.zeros(len(path_ct_stacks))\n",
    "\n",
    "idx = []\n",
    "for num, ct_path in enumerate(path_ct_stacks):\n",
    "    \n",
    "    fname = os.path.basename(ct_path['Image'])\n",
    "    idx_num = re.findall(r'\\d+', fname.split('.')[0])[0]\n",
    "    idx.append(int(idx_num))\n",
    "    \n",
    "    image, _ = nrrd.read(ct_path['Image'])\n",
    "    mask, _ = nrrd.read(ct_path['Mask'])\n",
    "    \n",
    "    image = (image - np.mean(image)) / (np.std(image) + 1e-12)    \n",
    "\n",
    "    cropped = image * mask\n",
    "    data = cropped.ravel()\n",
    "    \n",
    "    # Binning operation as conducted in PyRadiomics.\n",
    "    minimum = min(data)\n",
    "    maximum = max(data)\n",
    "\n",
    "    low_32_bound = minimum - (minimum % width32)\n",
    "    low_64_bound = minimum - (minimum % width64)\n",
    "    low_128_bound = minimum - (minimum % width128)\n",
    "\n",
    "    high_32_bound = maximum + 2 * width32\n",
    "    high_64_bound = maximum + 2 * width64\n",
    "    high_128_bound = maximum + 2 * width128\n",
    "\n",
    "    bin_32_edges = np.arange(low_32_bound, high_32_bound, width32)\n",
    "    bin_64_edges = np.arange(low_64_bound, high_64_bound, width64)\n",
    "    bin_128_edges = np.arange(low_128_bound, high_128_bound, width128)\n",
    "    \n",
    "    gl_32bins[num] = bin_32_edges\n",
    "    gl_64bins[num] = bin_64_edges\n",
    "    gl_128bins[num] = bin_128_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_32bins.shape, gl_64bins.shape, gl_128bins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average number of bins per GL discretization for the CURRENT images.\n",
    "np.mean(gl_32bins), np.mean(gl_64bins), np.mean(gl_128bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(gl_32bins), np.median(gl_64bins), np.median(gl_128bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(gl_32bins), np.std(gl_64bins), np.std(gl_128bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(gl_32bins), np.min(gl_64bins), np.min(gl_128bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(gl_32bins), np.max(gl_64bins), np.max(gl_128bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.squeeze(pd.read_csv(path_to_target, index_col=0).values)\n",
    "X = pd.read_csv(path_to_features, index_col=0)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_features = pd.read_csv('./../../data_source/to_analysis/clinical_params.csv', index_col=0)\n",
    "pet_params = pd.read_csv('./../../data_source/to_analysis/pet_params.csv', index_col=0)\n",
    "\n",
    "shape_features = X.filter(regex='shape')\n",
    "    \n",
    "clinical_features = clinical_features.loc[X.index, :]\n",
    "pet_params = pet_params.loc[X.index, :]\n",
    "\n",
    "shape_features.shape, clinical_features.shape, pet_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_feats = X.filter(regex='CT')\n",
    "PET_feats = X.filter(regex='PET')\n",
    "\n",
    "CT_feats.shape, PET_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_fo = CT_feats.filter(regex='firstorder')\n",
    "CT_fo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PET_fo = PET_feats.filter(regex='firstorder')\n",
    "PET_fo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_text = CT_feats.drop(CT_fo.columns, axis=1)\n",
    "CT_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PET_text = PET_feats.drop(PET_fo.columns, axis=1)\n",
    "# Skip PET parameter features.\n",
    "pet_param_cols = [col for col in PET_text.columns if 'PETparam' in col]\n",
    "PET_text.drop(pet_param_cols, axis=1, inplace=True)\n",
    "PET_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICC & Hassan Transform of CT Texture Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Hassan transform and record which features are transformed.\n",
    "gl_bins = [gl_32bins, gl_64bins, gl_128bins]\n",
    "df_ct_icc = calc_icc(CT_text, gl_bins)\n",
    "df_ct_icc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting original and transformed CT texture features.\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig = sns.barplot(\n",
    "    x=df_ct_icc.index, \n",
    "    y='Score', \n",
    "    hue='Kind', \n",
    "    data=df_ct_icc, \n",
    "    palette='muted',\n",
    ")\n",
    "plt.ylabel('Intraclass Correlation\\nCoefficient', fontsize=20)\n",
    "# Produces two sets of each label: one set for original, and one set for transformed \n",
    "# features.\n",
    "labels = np.unique(prep_labels(hassan_transforms.keys()))\n",
    "plt.xticks(\n",
    "    np.arange(len(labels)), labels,\n",
    "    rotation=45, ha='right', fontsize=17\n",
    ")\n",
    "plt.yticks(fontsize=17)\n",
    "\n",
    "for patch_num, patch in enumerate(fig.patches):\n",
    "    current_width = patch.get_width()\n",
    "    diff = current_width - 0.3\n",
    "    patch.set_width(0.3)\n",
    "    # Recenter bars.\n",
    "    patch.set_x(patch.get_x() + diff * 0.5)\n",
    "\n",
    "plt.axhline(y=ICC_THRESH, linestyle='--', alpha=0.5, color='darkblue')\n",
    "plt.legend(\n",
    "    fontsize=17,\n",
    "    title='Feature Definition:', title_fontsize=20,\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, 1.22),\n",
    "    ncol=2, \n",
    "    fancybox=True, \n",
    "    shadow=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('./../../figures/compressing_feature_space/orig_images_hassan_icc_ct_texture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICC & Hassan Transform PET Texture Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_bins = [gl_32bins, gl_64bins, gl_128bins]\n",
    "df_pet_icc = calc_icc(PET_text, gl_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting original and transformed CT texture features.\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig = sns.barplot(\n",
    "    x=df_pet_icc.index, \n",
    "    y='Score', \n",
    "    hue='Kind', \n",
    "    data=df_pet_icc, \n",
    "    palette='muted',\n",
    ")\n",
    "plt.ylabel('Intraclass Correlation Coefficient', fontsize=20)\n",
    "# Produces two sets of each label: one set for original, and one set for transformed \n",
    "# featuresm.\n",
    "labels = np.unique(prep_labels(hassan_transforms.keys(), pref='PET'))\n",
    "plt.xticks(\n",
    "    np.arange(len(labels)), labels,\n",
    "    rotation=45, ha='right', fontsize=17\n",
    ")\n",
    "plt.yticks(fontsize=17)\n",
    "\n",
    "for patch_num, patch in enumerate(fig.patches):\n",
    "    current_width = patch.get_width()\n",
    "    diff = current_width - 0.3\n",
    "    patch.set_width(0.3)\n",
    "    # Recenter bars.\n",
    "    patch.set_x(patch.get_x() + diff * 0.5)\n",
    "\n",
    "plt.axhline(y=ICC_THRESH, linestyle='--', alpha=0.5, color='darkblue')\n",
    "plt.legend(\n",
    "    fontsize=17,\n",
    "    title='Feature Definition:', title_fontsize=18,\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    ncol=2, \n",
    "    fancybox=True, \n",
    "    shadow=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('./../../figures/compressing_feature_space/orig_images_hassan_icc_pet_texture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Redundancy in PET and CT texture features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hassan_to_feat_space(X, to_transf, gl_bins, prefix):\n",
    "    \n",
    "    # Modified only the spcified features. Other features \n",
    "    # are retained in original version.\n",
    "    X_red = X.copy()\n",
    "    for cols_to_transf in to_transf:\n",
    "\n",
    "        feats = X.filter(regex=cols_to_transf)\n",
    "        tmp_feats = np.zeros_like(feats) \n",
    "        transform = hassan_transforms[cols_to_transf]\n",
    "        \n",
    "        for bin_num, (col, nbins) in enumerate(zip(feats.columns, gl_bins)):\n",
    "            tmp_feats[:, bin_num] = transform(nbins, feats.loc[:, col].values)\n",
    "            \n",
    "        if icc(feats.values) < icc(tmp_feats):\n",
    "            new_feat = np.mean(tmp_feats, axis=1)\n",
    "            X_red.loc[:, f'{prefix}_{cols_to_transf}'] = new_feat\n",
    "            X_red.drop(feats.columns, axis=1, inplace=True)\n",
    "        else:\n",
    "            print(f'Not transforming {cols_to_transf}')\n",
    "        \n",
    "    return X_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_transf_scores = df_ct_icc.loc[df_ct_icc.loc[:, 'Kind'] == 'Modified', 'Score']\n",
    "ct_to_modify = (ct_transf_scores > ICC_THRESH).index.values\n",
    "\n",
    "# Apply Hassan transform to features with ICC exceeding thresh. Drop redundant features.\n",
    "CT_text_red = hassan_to_feat_space(CT_text, ct_to_modify, gl_bins, prefix='CT')\n",
    "CT_text_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_transf_scores = df_pet_icc.loc[df_pet_icc.loc[:, 'Kind'] == 'Modified', 'Score']\n",
    "pet_to_modify = (pet_transf_scores > ICC_THRESH).index.values\n",
    "\n",
    "# Apply Hassan transform to features with ICC exceeding thresh. Drop redundant features.\n",
    "PET_text_red = hassan_to_feat_space(PET_text, pet_to_modify, gl_bins, prefix='PET')\n",
    "PET_text_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICC of Remaining PET & CT Texture & First Order Features\n",
    "\n",
    "The ICC may be used in further reduction of the feature space by calculating the ICC of each unique feature across image value discretizations, and remove those features with high correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_correlated_features(X, corr, prefix):\n",
    "    \n",
    "    X_red = X.copy()\n",
    "    for col in corr:\n",
    "        feats = X.filter(regex=col)\n",
    "        X_red.loc[:, f'{prefix}_{col}'] = np.mean(feats, axis=1)\n",
    "        X_red.drop(feats.columns, axis=1, inplace=True)\n",
    "        \n",
    "    return X_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_fo_col_regexes = column_regexes(CT_fo)\n",
    "\n",
    "ct_fo_icc = []\n",
    "for ct_fo_regex in ct_fo_col_regexes:\n",
    "    feats = CT_fo.filter(regex=ct_fo_regex)\n",
    "    if len(feats.columns) == 3:\n",
    "        ct_fo_icc.append(icc(feats.values))\n",
    "    else:\n",
    "        ct_fo_icc.append(0.0)\n",
    "        \n",
    "ct_fo_icc = np.array(ct_fo_icc)\n",
    "ct_fo_to_mod = ct_fo_col_regexes[ct_fo_icc > ICC_THRESH]\n",
    "CT_fo_red = combine_correlated_features(CT_fo, ct_fo_to_mod, prefix='CT')\n",
    "CT_fo_red.index = X.index\n",
    "CT_fo_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_text_red_col_regexes = column_regexes(CT_text_red)\n",
    "\n",
    "ct_text_red_icc = []\n",
    "for ct_text_regex in ct_text_red_col_regexes:\n",
    "    feats = CT_text_red.filter(regex=ct_text_regex)\n",
    "    if len(feats.columns) == 3:\n",
    "        ct_text_red_icc.append(icc(feats.values))\n",
    "    else:\n",
    "        ct_text_red_icc.append(0.0)\n",
    "        \n",
    "ct_text_red_icc = np.array(ct_text_red_icc)\n",
    "ct_text_red_to_mod = ct_text_red_col_regexes[ct_text_red_icc > ICC_THRESH]\n",
    "CT_text_red = combine_correlated_features(CT_text_red, ct_text_red_to_mod, prefix='CT')\n",
    "CT_text_red.index = X.index\n",
    "CT_text_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_fo_col_regexes = column_regexes(PET_fo)\n",
    "\n",
    "pet_fo_icc = []\n",
    "for pet_fo_regex in pet_fo_col_regexes:\n",
    "    feats = PET_fo.filter(regex=pet_fo_regex)\n",
    "    if len(feats.columns) == 3:\n",
    "        pet_fo_icc.append(icc(feats.values))\n",
    "    else:\n",
    "        pet_fo_icc.append(0.0)\n",
    "    \n",
    "pet_fo_icc = np.array(pet_fo_icc)\n",
    "pet_fo_to_mod = pet_fo_col_regexes[pet_fo_icc > ICC_THRESH]\n",
    "PET_fo_red = combine_correlated_features(PET_fo, pet_fo_to_mod, prefix='PET')\n",
    "PET_fo_red.index = X.index\n",
    "PET_fo_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_text_red_col_regexes = column_regexes(PET_text_red)\n",
    "\n",
    "pet_text_red_icc = []\n",
    "for pet_text_regex in pet_text_red_col_regexes:\n",
    "    feats = PET_text_red.filter(regex=pet_text_regex)\n",
    "    if len(feats.columns) == 3:\n",
    "        pet_text_red_icc.append(icc(feats.values))\n",
    "    else:\n",
    "        pet_text_red_icc.append(0.0)\n",
    "        \n",
    "pet_text_red_icc = np.array(pet_text_red_icc)\n",
    "pet_text_red_to_mod = pet_text_red_col_regexes[pet_text_red_icc > ICC_THRESH]\n",
    "PET_text_red = combine_correlated_features(PET_text_red, pet_text_red_to_mod, prefix='PET')\n",
    "PET_text_red.index = X.index\n",
    "PET_text_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = pd.concat(\n",
    "    (\n",
    "        #clinical_features, \n",
    "        shape_features, \n",
    "        CT_fo_red,\n",
    "        CT_text_red, \n",
    "        PET_fo_red,\n",
    "        PET_text_red, \n",
    "        pet_params, \n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "X_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red.to_csv(path_to_icc)\n",
    "X_red.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_red: Removed featues with ICC > 0.8.\n",
    "red_corr_matrix = X_red.corr(method='spearman').abs()\n",
    "upper_red = red_corr_matrix.where(np.triu(np.ones(red_corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_red.columns if any(upper_red[column] > 0.95)]\n",
    "X_red_dropped = X_red.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_red_dropped: Removed featues with ICC > 0.8 and features with SCC > 0.95.\n",
    "red_dropped_corr_matrix = X_red_dropped.corr(method='spearman').abs()\n",
    "upper_red_dropped = red_dropped_corr_matrix.where(\n",
    "    np.triu(np.ones(red_dropped_corr_matrix.shape), k=1).astype(np.bool)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get largest correlation between two features.\n",
    "red_max_feature_corr = []\n",
    "for num, col in enumerate(upper_red.columns):\n",
    "    if not np.isnan(max(upper_red[col])):\n",
    "        red_max_feature_corr.append(max(upper_red[col]))\n",
    "\n",
    "red_max_feature_corr = np.array(red_max_feature_corr)\n",
    "\n",
    "red_dropped_max_feature_corr = []\n",
    "for num, col in enumerate(upper_red_dropped.columns):\n",
    "    if not np.isnan(max(upper_red_dropped[col])):\n",
    "        red_dropped_max_feature_corr.append(max(upper_red_dropped[col]))\n",
    "\n",
    "red_dropped_max_feature_corr = np.array(red_dropped_max_feature_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum Spearman's Correlation Coefficient a feature associates with\n",
    "# another feature from the Hassan transformed feature matrix. That is, max SCC of features\n",
    "# after the Hassan transforms.\n",
    "\n",
    "red_feature_cats = np.array(to_feature_categories(upper_red.columns))\n",
    "sorted_cats_idx = np.argsort(red_feature_cats)\n",
    "\n",
    "to_drop = np.where(sorted_cats_idx == np.max(sorted_cats_idx))\n",
    "sorted_cats_idx = np.delete(sorted_cats_idx, to_drop)\n",
    "red_feature_cats = np.delete(red_feature_cats, to_drop)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(\n",
    "    np.arange(np.size(red_max_feature_corr)), \n",
    "    red_max_feature_corr[sorted_cats_idx],\n",
    "    hue=red_feature_cats[sorted_cats_idx],\n",
    "    palette=sns.color_palette('muted', len(np.unique(red_feature_cats))),\n",
    "    s=50\n",
    ")\n",
    "plt.legend(\n",
    "    fontsize=17, title_fontsize=20, title='Feature Category', loc='upper center', \n",
    "    bbox_to_anchor=(0.5, 0.3), ncol=3, fancybox=True, shadow=True\n",
    ")\n",
    "plt.ylabel(\"Maximum Associated\\nSpearman's Correaltion Coefficient\", fontsize=20)\n",
    "plt.xlabel('Feature Indicator', fontsize=20)\n",
    "plt.xticks(\n",
    "    np.linspace(0, np.size(red_max_feature_corr), 6, dtype=int), \n",
    "    np.linspace(1, np.size(red_max_feature_corr), 6, dtype=int), \n",
    "    fontsize=17\n",
    ")\n",
    "plt.yticks(fontsize=17)\n",
    "\n",
    "plt.savefig(\n",
    "    './../../figures/compressing_feature_space/scc_after_icc_radiom_feats.png', \n",
    "    bbox_inches='tight',\n",
    "    dpi=100, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum Spearman's Correlation Coefficient a feature associates with\n",
    "# another feature from the Hassan transformed feature matrix including removal of \n",
    "# features with maximum SCC > 0.95.\n",
    "red_dropped_feature_cats = np.array(to_feature_categories(upper_red_dropped.columns))\n",
    "sorted_cats_idx = np.argsort(red_dropped_feature_cats)\n",
    "\n",
    "to_drop = np.where(sorted_cats_idx == np.max(sorted_cats_idx))\n",
    "sorted_cats_idx = np.delete(sorted_cats_idx, to_drop)\n",
    "red_dropped_feature_cats = np.delete(red_dropped_feature_cats, to_drop)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(\n",
    "    np.arange(np.size(red_dropped_max_feature_corr)), \n",
    "    red_dropped_max_feature_corr[sorted_cats_idx],\n",
    "    hue=red_dropped_feature_cats[sorted_cats_idx],\n",
    "    palette=sns.color_palette('muted', len(np.unique(red_dropped_feature_cats))),\n",
    "    s=50\n",
    ")\n",
    "plt.legend(\n",
    "    fontsize=17, title_fontsize=20, title='Feature Category', loc='upper center', \n",
    "    bbox_to_anchor=(0.6, 0.3), ncol=3, fancybox=True, shadow=True\n",
    ")\n",
    "plt.ylabel(\"Maximum Associated\\nSpearman's Correaltion Coefficient\", fontsize=20)\n",
    "plt.xlabel('Feature Indicator', fontsize=20)\n",
    "plt.xticks(\n",
    "    np.linspace(0, np.size(red_dropped_max_feature_corr), 6, dtype=int), \n",
    "    np.linspace(1, np.size(red_dropped_max_feature_corr), 6, dtype=int), \n",
    "    fontsize=17\n",
    ")\n",
    "plt.yticks(np.linspace(0.0, 1.0, 6), np.round(np.linspace(0.0, 1.0, 6), 1), fontsize=17)\n",
    "plt.savefig(\n",
    "    './../../figures/compressing_feature_space/orig_images_hassan_icc_scc_dropped_radiom_feats.png', \n",
    "    bbox_inches='tight',\n",
    "    dpi=100, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Include clinical variables!\n",
    "#X_red_dropped.to_csv(path_red_dropped)\n",
    "#X_red_dropped.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
